{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le basi del Dataframe\n",
    "In questo notebook impareremo ad utilizzare il modulo SparkSQL insieme al Dataframe per processare dati strutturati in parallelo. Il Dataframe è un'astrazione del RDD che permette di organizzare i dati in colonne ed eseguire operazioni su di esse utilizzando l'ottimizzazione del modulo SparkSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizializzazione di Spark\n",
    "Vediamo un modo differente per inizializzare Spark: la SparkSession, che ci permette di creare con un unica funzione il contesto per Spark (SparkContext) ed il contesto per SQL (SQLContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('basic').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare un Dataframe\n",
    "Possiamo creare un nuovo Dataframe usando il metodo *.createDataFrame(data, names)* dell'oggetto SparkSession, questo metodo ha bisogno di due parametri:\n",
    "\n",
    "* Una lista di tuple, in cui ogni tupla corrisponde ad una riga del Dataframe.\n",
    "* Una lista con i nomi per le colonne\n",
    "\n",
    "Creiamo un Dataframe contenente, nome, sesso, età, altezza e peso di 5 persone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(\"Giuseppe\", \"M\", 23, 174, 70.5),\n",
    "        (\"Antonio\", \"M\", 25, 179, 68.),\n",
    "        (\"Lorenzo\", \"M\", 33, 172, 88.5),\n",
    "        (\"Luisa\", \"F\", 48, 155, 50.2),\n",
    "        (\"Margheria\", \"F\", 35, 165, 54.3)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"gender\", \"age\", \"height\",\"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per visualizzare il Dataframe posssiamo usare il metodo *.show()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+------+\n",
      "|     name|gender|age|height|weight|\n",
      "+---------+------+---+------+------+\n",
      "| Giuseppe|     M| 23|   174|  70.5|\n",
      "|  Antonio|     M| 25|   179|  68.0|\n",
      "|  Lorenzo|     M| 33|   172|  88.5|\n",
      "|    Luisa|     F| 48|   155|  50.2|\n",
      "|Margheria|     F| 35|   165|  54.3|\n",
      "+---------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*.show()* stamperà fino ad un massimo di 20 righe, se vogliamo stampare un numero esatto di righe basta passare il numero come parametro del metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+------+------+\n",
      "|    name|gender|age|height|weight|\n",
      "+--------+------+---+------+------+\n",
      "|Giuseppe|     M| 23|   174|  70.5|\n",
      "| Antonio|     M| 25|   179|  68.0|\n",
      "| Lorenzo|     M| 33|   172|  88.5|\n",
      "+--------+------+---+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per conoscere il numero esatto di righe presenti all'interno del Dataframe possiamo usare il metodo *.count()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per stampare i nomi delle colonne possiamo usare l'attributo *.columns*, che ritornerà una lista con i nomi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'gender', 'age', 'height', 'weight']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per stampare lo schema del Dataframe, cioè le informazioni legate ad ogni attributo (nome, tipo, se può essere null), possiamo usare il metodo *.printSchema()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infinie possiamo visualizzare una serie di informazioni statistiche (count, valore medio, deviazione standard, valore minimo e massimo) usando il metodo *.describe()*, il quale ci ritornerà un Dataframe contentente queste informazioni, che possiamo quindi visualizzare usando il metodo *.show()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-----------------+-----------------+-----------------+\n",
      "|summary|     name|gender|              age|           height|           weight|\n",
      "+-------+---------+------+-----------------+-----------------+-----------------+\n",
      "|  count|        5|     5|                5|                5|                5|\n",
      "|   mean|     null|  null|             32.8|            169.0|             66.3|\n",
      "| stddev|     null|  null|9.909591313469994|9.300537618869138|15.13753612712452|\n",
      "|    min|  Antonio|     F|               23|              155|             50.2|\n",
      "|    max|Margheria|     M|               48|              179|             88.5|\n",
      "+-------+---------+------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificare lo schema\n",
    "Come vedi lo schema del Dataframe è stato estratto direttamente dai dati, ma se volessimo definirlo noi ? Ad esempio, mettiamo caso di voler cambiare il tipo per le seguenti colonne:\n",
    "* **age**: da long a intero.\n",
    "* **height**: da long a intero.\n",
    "* **weight**: da double a float.\n",
    "\n",
    "Possiamo farlo creando uno schema per poi passarlo al metodo *.createDataFrame(data, schema)*.\n",
    "Vediamo come fare, per prima cosa creiamo i campi dello schema, utilizzando la classe *StructField*, a cui passeremo questi parametri:\n",
    "* nome della colonna\n",
    "* tipo della colonna (deve essere l'istanza di una classe reperibile dal modulo pyspark.sql.types), a [questo link](https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/sql/types/DataTypes.html) puoi trovare la classe per ogni tipo di dato.\n",
    "* un valore booleano che indica se tale valore può essere lasciato vuoto (null) oppure no.\n",
    "\n",
    "Infine per costruire lo schema utilizziamo la classe *StructType*, passando la lista di campi che abbiamo definitp sopra al parametro fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [StructField('name', StringType(), True),\n",
    "                StructField('gender', StringType(), True),\n",
    "                StructField('age', IntegerType(), True),\n",
    "                StructField('height', IntegerType(), True),\n",
    "                StructField('weight', FloatType(), True)]\n",
    "            \n",
    "schema = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso creiamo il Dataframe, passando i dati e lo schema all'interno del parametro schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+------+\n",
      "|     name|gender|age|height|weight|\n",
      "+---------+------+---+------+------+\n",
      "| Giuseppe|     M| 23|   174|  70.5|\n",
      "|  Antonio|     M| 25|   179|  68.0|\n",
      "|  Lorenzo|     M| 33|   172|  88.5|\n",
      "|    Luisa|     F| 48|   155|  50.2|\n",
      "|Margheria|     F| 35|   165|  54.3|\n",
      "+---------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il risultato sembrerebbe essere lo stesso di prima, ma vediamo lo schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- weight: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso i tipi sono quelli definiti da noi :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Righe e Colonne\n",
    "Possiamo ottenere n righe del Dataframe utilizzando il metodo *.head(n)* il risultato sarà un lista di oggetti Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Giuseppe', gender='M', age=23, height=174, weight=70.5),\n",
       " Row(name='Antonio', gender='M', age=25, height=179, weight=68.0),\n",
       " Row(name='Lorenzo', gender='M', age=33, height=172, weight=88.5),\n",
       " Row(name='Luisa', gender='F', age=48, height=155, weight=50.20000076293945),\n",
       " Row(name='Margheria', gender='F', age=35, height=165, weight=54.29999923706055)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per selezionare solo una colonna del Dataframe possiamo usare l'indice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'name'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...oppure il nome, in ogni caso il risultato sarà un oggetto Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'name'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raramente ti capiterà di lavorare direttamente con righe e colonne, piuttosto per selezionare soltanto una colonna puoi usare il metodo *.select(name)*, che ti ritonernà un Dataframe contenente soltanto la colonna selezionata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "| Giuseppe|\n",
      "|  Antonio|\n",
      "|  Lorenzo|\n",
      "|    Luisa|\n",
      "|Margheria|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfName = df.select(\"name\")\n",
    "dfName.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo usare lo stesso metodo per selezionare più colonne, passando una lista di nomi come parametro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "| Giuseppe| 23|\n",
      "|  Antonio| 25|\n",
      "|  Lorenzo| 33|\n",
      "|    Luisa| 48|\n",
      "|Margheria| 35|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creare e modificare colonne\n",
    "Possiamo modificare una determinata colonna utilizzando il metodo *.withColumn(name, column)*. alla quale dovremo passare il nome della riga che dovremmo modificare e un oggetto colonna con il nuovo valore. Ad esempio modifichiamo la colonna height per convertire i centimentri in metr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+------+\n",
      "|     name|gender|age|height|weight|\n",
      "+---------+------+---+------+------+\n",
      "| Giuseppe|     M| 23|  1.74|  70.5|\n",
      "|  Antonio|     M| 25|  1.79|  68.0|\n",
      "|  Lorenzo|     M| 33|  1.72|  88.5|\n",
      "|    Luisa|     F| 48|  1.55|  50.2|\n",
      "|Margheria|     F| 35|  1.65|  54.3|\n",
      "+---------+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"height\", df[\"height\"]/100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampiamo lo schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vedi lo schema è stato modificato automaticamente, dato che abbiamo convertito l'altezza da numeri interi a numeri con la virgola. Se la colonna che stiamo tentando di modificare non esiste, verrà creata. Ad esempio proviamo a creare una colonna contenente l'indice di massa corporea (bmi) di ogni persona, calcolandolo come:\n",
    "<br><br>\n",
    "$$ bmi = (\\frac{weight}{height})^2 $$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+------+------------------+\n",
      "|     name|gender|age|height|weight|               bmi|\n",
      "+---------+------+---+------+------+------------------+\n",
      "| Giuseppe|     M| 23|  1.74|  70.5|23.285770907649624|\n",
      "|  Antonio|     M| 25|  1.79|  68.0| 21.22280827689523|\n",
      "|  Lorenzo|     M| 33|  1.72|  88.5| 29.91481882098432|\n",
      "|    Luisa|     F| 48|  1.55|  50.2| 20.89490146220164|\n",
      "|Margheria|     F| 35|  1.65|  54.3|19.944903301032344|\n",
      "+---------+------+---+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bmi = df[\"weight\"]/(df[\"height\"]**2)\n",
    "df = df.withColumn(\"bmi\", bmi)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori del bmi contengono molte cifre dopo la virgola, meglio approssimarle, possiamo farlo utilizzando la funzione *round(col, val)* di spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+------+-----+\n",
      "|     name|gender|age|height|weight|  bmi|\n",
      "+---------+------+---+------+------+-----+\n",
      "| Giuseppe|     M| 23|  1.74|  70.5|23.29|\n",
      "|  Antonio|     M| 25|  1.79|  68.0|21.22|\n",
      "|  Lorenzo|     M| 33|  1.72|  88.5|29.91|\n",
      "|    Luisa|     F| 48|  1.55|  50.2|20.89|\n",
      "|Margheria|     F| 35|  1.65|  54.3|19.94|\n",
      "+---------+------+---+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df = df.withColumn(\"bmi\", round(df[\"bmi\"], 2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Molto meglio ! Un'altra funzione utile è *when(condition, value)*, che ci permette di modificare le colonne in base a determinate condizioni. Ad esempio creiamo una nuova colonna chiamata *is_fat* che conterrà un valore true se la persona è in sovrappeso (bmi>25), False altrimenti. Possiamo gestire 'l'altrimenti' utilizzando la funzione *otherwise(value)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+------+-----+------+\n",
      "|     name|gender|age|height|weight|  bmi|is_fat|\n",
      "+---------+------+---+------+------+-----+------+\n",
      "| Giuseppe|     M| 23|  1.74|  70.5|23.29| false|\n",
      "|  Antonio|     M| 25|  1.79|  68.0|21.22| false|\n",
      "|  Lorenzo|     M| 33|  1.72|  88.5|29.91|  true|\n",
      "|    Luisa|     F| 48|  1.55|  50.2|20.89| false|\n",
      "|Margheria|     F| 35|  1.65|  54.3|19.94| false|\n",
      "+---------+------+---+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df = df.withColumn(\"is_fat\", when(col(\"bmi\")>25, True).otherwise(False))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ottimo ! Per finire rinominiamo la colonna *gender* in *sex*, possiamo farlo tramite il metodo *.withColumnRenamed(old_name, new_name)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+------+------+-----+------+\n",
      "|     name|sex|age|height|weight|  bmi|is_fat|\n",
      "+---------+---+---+------+------+-----+------+\n",
      "| Giuseppe|  M| 23|  1.74|  70.5|23.29| false|\n",
      "|  Antonio|  M| 25|  1.79|  68.0|21.22| false|\n",
      "|  Lorenzo|  M| 33|  1.72|  88.5|29.91|  true|\n",
      "|    Luisa|  F| 48|  1.55|  50.2|20.89| false|\n",
      "|Margheria|  F| 35|  1.65|  54.3|19.94| false|\n",
      "+---------+---+---+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"gender\",\"sex\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtri\n",
    "Possiamo filtrare le righe del Dataframe utilizzando il metodo *.filter(condition)*. Ad esempio filtriamo soltanto gli uomini, possiamo scrivere la condizione come una stringa..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------+------+-----+------+\n",
      "|    name|sex|age|height|weight|  bmi|is_fat|\n",
      "+--------+---+---+------+------+-----+------+\n",
      "|Giuseppe|  M| 23|  1.74|  70.5|23.29| false|\n",
      "| Antonio|  M| 25|  1.79|  68.0|21.22| false|\n",
      "| Lorenzo|  M| 33|  1.72|  88.5|29.91|  true|\n",
      "+--------+---+---+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_male = df.filter(\"sex == 'M'\")\n",
    "df_male.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...oppure usando la colonna, il risultato sarà lo stesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+------+------+-----+------+\n",
      "|    name|sex|age|height|weight|  bmi|is_fat|\n",
      "+--------+---+---+------+------+-----+------+\n",
      "|Giuseppe|  M| 23|  1.74|  70.5|23.29| false|\n",
      "| Antonio|  M| 25|  1.79|  68.0|21.22| false|\n",
      "| Lorenzo|  M| 33|  1.72|  88.5|29.91|  true|\n",
      "+--------+---+---+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_male = df.filter(df[\"sex\"] == 'M')\n",
    "df_male.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregazione\n",
    "Possiamo aggregare un Dataframe in base al contenuto di una colonna usando il metodo *.groupBy(col)*. Ad esempio aggreghiamo il Dataframe in base al sesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.group.GroupedData"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = df.groupBy('sex')\n",
    "type(df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il risultato sarà un'oggetto GroupedData, sulla quale possiamo eseguire diverse operazioni aritmetiche e statistiche, come conteggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|sex|count|\n",
      "+---+-----+\n",
      "|  F|    2|\n",
      "|  M|    3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+-----------------+-----------------+\n",
      "|sex|avg(age)|avg(height)|      avg(weight)|         avg(bmi)|\n",
      "+---+--------+-----------+-----------------+-----------------+\n",
      "|  F|    41.5|        1.6|            52.25|           20.415|\n",
      "|  M|    27.0|       1.75|75.66666666666667|24.80666666666667|\n",
      "+---+--------+-----------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "somma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+-----------+--------+\n",
      "|sex|sum(age)|sum(height)|sum(weight)|sum(bmi)|\n",
      "+---+--------+-----------+-----------+--------+\n",
      "|  F|      83|        3.2|      104.5|   40.83|\n",
      "|  M|      81|       5.25|      227.0|   74.42|\n",
      "+---+--------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valore massimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+-----------+--------+\n",
      "|sex|max(age)|max(height)|max(weight)|max(bmi)|\n",
      "+---+--------+-----------+-----------+--------+\n",
      "|  F|      48|       1.65|       54.3|   20.89|\n",
      "|  M|      33|       1.79|       88.5|   29.91|\n",
      "+---+--------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e valore minimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+-----------+--------+\n",
      "|sex|min(age)|min(height)|min(weight)|min(bmi)|\n",
      "+---+--------+-----------+-----------+--------+\n",
      "|  F|      35|       1.55|       50.2|   19.94|\n",
      "|  M|      23|       1.72|       68.0|   21.22|\n",
      "+---+--------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.min().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo anche operare su singole colonne usando il metodo *.agg(op)* del Dataframe, che prende come parametro un dizionario contenente nome della colonna e operazione da eseguire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(weight)|\n",
      "+-----------+\n",
      "|      331.5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'weight':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo utilizzare questo stesso metodo per eseguire operazioni differenti su colonne differenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-----------+\n",
      "|sex|count(sex)|sum(weight)|max(height)|\n",
      "+---+----------+-----------+-----------+\n",
      "|  F|         2|      104.5|       1.65|\n",
      "|  M|         3|      227.0|       1.79|\n",
      "+---+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg({'weight':'sum', 'height':'max', 'sex':'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vedi il nome delle nuove colonne viene assegnato automaticamente, in base alla funzione ed alla colonna che abbiamo utilizzato, possiamo modificare tali nomi usando il metodo *.withColumnRenamed(old_name, new_name)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+----------+\n",
      "|sex|count_sex|sum_weight|max_height|\n",
      "+---+---------+----------+----------+\n",
      "|  F|        2|     104.5|      1.65|\n",
      "|  M|        3|     227.0|      1.79|\n",
      "+---+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group.agg({'weight':'sum', 'height':'max', 'sex':'count'}).withColumnRenamed(\"count(sex)\",\"count_sex\").withColumnRenamed(\"sum(weight)\",\"sum_weight\").withColumnRenamed(\"max(height)\",\"max_height\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piuttosto che un dizionario, possiamo anche utilizzare delle funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+----------+\n",
      "|sex|sum(weight)|max(height)|count(sex)|\n",
      "+---+-----------+-----------+----------+\n",
      "|  F|      104.5|       1.65|         2|\n",
      "|  M|      227.0|       1.79|         3|\n",
      "+---+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, max, count\n",
    "\n",
    "df_group.agg(sum(\"weight\"), max('height'), count('sex')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso per settare arbitrariamente i nomi delle colonne possiamo creare un alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---------+\n",
      "|sex|sum_weight|max_height|count_sex|\n",
      "+---+----------+----------+---------+\n",
      "|  F|     104.5|      1.65|        2|\n",
      "|  M|     227.0|      1.79|        3|\n",
      "+---+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, max, count\n",
    "\n",
    "df_group.agg(sum(\"weight\").alias(\"sum_weight\"), max('height').alias(\"max_height\"), count('sex').alias(\"count_sex\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ordinare un Dataframe possiamo utilizzare il metodo *.orderBy(col)*, ad esempio ordiniamo in base al peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+------+------+-----+------+\n",
      "|     name|sex|age|height|weight|  bmi|is_fat|\n",
      "+---------+---+---+------+------+-----+------+\n",
      "|    Luisa|  F| 48|  1.55|  50.2|20.89| false|\n",
      "|Margheria|  F| 35|  1.65|  54.3|19.94| false|\n",
      "|  Antonio|  M| 25|  1.79|  68.0|21.22| false|\n",
      "| Giuseppe|  M| 23|  1.74|  70.5|23.29| false|\n",
      "|  Lorenzo|  M| 33|  1.72|  88.5|29.91|  true|\n",
      "+---------+---+---+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"weight\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di default l'ordinamento viene eseguito in maniera ascendente (dal valore minore al valore maggiore), per eseguirlo in maniera discendente ci basta impostare il parametro *ascending* a False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+------+------+-----+------+\n",
      "|     name|sex|age|height|weight|  bmi|is_fat|\n",
      "+---------+---+---+------+------+-----+------+\n",
      "|  Lorenzo|  M| 33|  1.72|  88.5|29.91|  true|\n",
      "| Giuseppe|  M| 23|  1.74|  70.5|23.29| false|\n",
      "|  Antonio|  M| 25|  1.79|  68.0|21.22| false|\n",
      "|Margheria|  F| 35|  1.65|  54.3|19.94| false|\n",
      "|    Luisa|  F| 48|  1.55|  50.2|20.89| false|\n",
      "+---------+---+---+------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"weight\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link utili e approfondimenti\n",
    "* [Documentazione del modulo SQL e del DataFrame di pySpark](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)\n",
    "* [Differenza tra SparkContext e SparkSession (attezione: esempi in Scala)](https://medium.com/knoldus/spark-why-should-we-use-sparksession-fdebe864d895)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
